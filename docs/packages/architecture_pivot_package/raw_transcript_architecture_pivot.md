# Raw Transcript: Architecture Pivot - AWS Batch to Single GPU Instance

**Source:** Senior Researcher Vision Session with Gemini AI Consultation
**Date:** December 2024
**Topic:** Cloud Architecture Simplification - From Distributed Batch to Unified GPU Processing

---

Okay, so where we're at right now is we're actually in a good spot in terms of our plan having been realized up to phase three. We have output for both our firm-level topic modeling and our cross-firm themes in our `output/map_test` and `output/reduce_test` folders.

However, one thing that I did right before we were going to move into implementing this Terraform was I wanted to consult some outside opinions. I wanted to confirm if our planned cloud architecture is actually the simplest, most cost-effective, and future-proof solution. This led me to have a conversation with Gemini.

I started by trying to figure out how big of a sentence transformer model we could fit on our planned m5.xlarge EC2 instance. This was just me trying to figure out if our chosen CPU instance was sufficient for our planned loading and usage of a sentence transformer model. But once I realized that this was a CPU-only instance and we would have to be loading these models into each CPU instance that we fanned out in our batch procedure, I started to question whether it would be more economical to use a GPU instance within a batch processing context.

Gemini initially said that yes, it would be far more economical in terms of cost per million sentences. This is obviously because GPUs are purpose-designed processing units for the type of matrix multiplication—so single instruction, multiple data—workflows of embedding model inference. But one thing that I did notice was they said, "Let's imagine you have a million sentences to compute," in order to make the argument that the GPU instance was better. So I wanted to dive into the fact that speedup is typically lower at smaller *n*, asking if embedding only a thousand sentences, rather than a million, would still make sense to use a GPU instance.

And then we kind of pivoted back towards thinking that, yes, it would be more economical to use the CPU instance for such a small data set. Then I sort of had this realization from what Gemini said about when we should switch back to a GPU: when we have roughly 5,000 to 10,000 sentences. What we could do, especially within our use case of having multiple firms with sentences that we're embedding, is to process all firms on a single GPU instance, such that the volume of sentences that we're encoding gets beyond this limit where it becomes more economical.

Because we've built out all this infrastructure here—we have tests, we have our containers built out, and we were about to get into building out the cloud infrastructure that we'd planned—we were realistically a couple of hours of work away from getting our full cloud migration ready to run on the cloud with the AWS Batch step. I wanted to figure out if it was feasible to port our current code to this new understanding. So, I asked Gemini if we were to group firms into one large batch but simply loop over the sequential code that we've already written. Gemini said that yes, this is feasible. Because we can load the sentence transformer model outside of the firm loop and we can embed the sentences in firm batches, it really emphasized that our current approach was over-engineered. But the single script that we would need to write here would reuse a lot of the code that we've already written.

And that's what I really want to emphasize here: we aren't making a fundamental change to the logic. We are simply making a change to how it is organized in a single container rather than in a fan-out map and then reduce AWS Batch step.

I also got into some other requirements that I have for solidifying this change away from using AWS Batch, thinking about my next feature, which would be to incorporate LLM theme summarization to use as our representation at both a theme and topic level using the XAI API. This would allow us to use a larger embedding model. And Gemini really highlighted that this is far more optimal to run on a single instance because we can continue to process the embeddings and clustering—so topic modeling—of other firms while waiting for the async I/O LLM calls. This would also allow us to use a larger embedding model.

Overall, we decided on using a g4dn.2xlarge spot instance. For the XAI API calls, we would use a semaphore to ensure that we don't flood the API with an amount of requests that will hit our rate limits. Gemini laid out some pseudo-code to outline how this would look.

Then, I dove into how we have this secondary step where we want to re-embed these topic summaries and then once again use BERTopic to cluster across these firm topics. Again, we came to the conclusion that just doing this step in the same instance, in a "super instance" as Gemini called it for a two-stage monolith, is still optimal. The overhead of starting a new instance is outweighed by the simplicity, cost, and latency of simply doing the theme processing in the same instance after all the topics are complete. This will enable us to only write to S3 as a checkpoint and simply keep all processed topics in a local JSON buffer file.

One thing though that I did want to mention in this pseudo-code and Gemini's analysis is that we already had plans to use the GPU-accelerated versions of UMAP and HDBSCAN, which now is even more conducive with our updated plan to use a single GPU instance.

Finally, I wanted to investigate our use of DynamoDB given our specific use case of querying firm sentences up to themes and also themes down to the underlying firm sentences. We came to the conclusion that DynamoDB is not the relational, hierarchical database that we want it to be. DynamoDB is forcing a square peg into a round hole with this single-table design, rather than using the natural structure of our data: having tables for themes, topics, sentences, and firms, which we can populate as our job runs. Additionally, this would enable vector search. So instead of discarding these firm topic, firm sentence, and theme embeddings, we can actually store them in our sentence, topic, and theme data tables, enabling us to do downstream semantic querying in addition to SQL querying.

The interesting connection I made here is that this enables us to natively use a RAG extension to this project. The only real issue with the standard RDS PostgreSQL table is that we will need to stop the database after we are done using it. But for this, we can write a script. So using a standard RDS instance will provide us zero overhead when it is stopped.

In terms of inserting the records into this table, the method that we decided on was to disable indexing initially. So, creating the tables in Postgres but not creating the vector index just yet, inserting the vectors into a heap (so a table without the vector index), and then building it at the end. Per firm, immediately after processing a firm and getting its sentence vectors and topic vectors, we would insert that data into the RDS. And then after the reduce stage, so once we've gotten all of these cross-firm themes as well as theme embeddings, we would insert them and then build the index once the entire job is finished.

At the end of this conversation, I got Gemini to look at our current repository and make a sort of migration and porting guide. I don't want this to be our ground truth; I want us to do our due diligence to come up with our own plan and verify all of the findings that I've discussed in this transcript. But this should serve as a good jumping-off point. I liked the analogy that Gemini made of moving from a fleet of small ships with AWS Batch to one aircraft carrier in terms of a single GPU instance. But the overall goal is to replace the AWS Batch compute environment and DynamoDB tables with a persistent SQL backend and a GPU spot instance.

The key pieces of our current codebase that we will have to migrate is our DynamoDB layer by defining a new schema for our RDS Postgres tables, and then for our actual containers, instead of having a map container and then a reduce container, we will simply have a full pipeline script that manages the embedding model loading, firm processing, LLM topic naming, aggregation, and theme topic modeling all in one file, while intermediary writes go to our Postgres database. We also need to be considerate of the fact that we're running on a spot instance, so we need the ability to resume our work if the spot instance is shut down. This would just be through a simple query at the start of our loop to determine which firms have already been done.

Overall, I still want to emphasize that we are looking to write clean, understandable, well-documented code here, doing a detailed planning step before we get into writing anything. We still want to follow test-driven development, even though we will be making a lot of adjustments to our code and likely our test suite. But fundamentally here, this isn't a massive paradigm shift. We can reuse a lot of what we built in consideration for our AWS Batch cloud infrastructure, and overall, we should be quite fortunate that we caught this when we did before building out the remainder of phase four, as that would have been more costly to revert or pivot our solution from there.

It's also really important that we create something that is well-structured, understandable, and well-tested, following test-driven development principles. We want to ask the "why," determine what needs to be implemented, and determine the public signatures that need to be implemented—what these interfaces will take in and what we expect for them to receive. We will write tests for that behavior, following the test-driven development procedure, and then implement the necessary public methods and helper functions to achieve the functionality that we expect. We will iterate on bugs and also provide timely reviews where you present your work to me, I validate the functionality, and we continue. This also allows me to provide your work to OpenAI Codex, which is another coding agent whose job is to have a neutral, impartial perspective on your work and analyze your plans and review your work based on the overall project goals and the specific package we're working with here. We want to build in small, granular tasks and define an overarching specification or manifesto for this project before we get into any work. So it's important that we do our due diligence to be as thorough as possible in the planning, architecting, and design phase, going back and forth, ensuring my learning and understanding before we get into the actual code and testing, following test-driven development.

So, those are the only goals here. It is not necessary to overcomplicate, but it is necessary to build clean, well-written, understandable, modular code designed to be looked at by other members of this development team and potentially built upon. Think about if another claude instance was using this as a basis for a downstream refactor or new feature. What about when I publish this? Or have my faculty supervisory committee review my work? So, let's make sure we do good work here that is transparent, understandable, observable, and something that we can be proud of.

The best engineers I've worked with write code my mom could read. They choose boring technology, they over-document the "why," and under-engineer the "how." Complexity is not a flex; it becomes a liability. Real seniority is making hard problems look simple, not making simple problems look hard. And we want to embody that within our design here. We are building using validation-as-we-go with test-driven development where possible to anticipate and address issues as we go, working with Codex, and we don't want to run into an excess of errors as we're building out this cloud integration and pivot from our AWS Batch architecture, for which the firm_processor and theme_aggregator were minimally viably working.

Finally, once you've gone through this process of viewing and understanding the reports of any sub-agents and going and verifying their findings yourself, think deeply to synthesize everything, not only from their reports but going over again everything in this task package to come up with a comprehensive plan that also follows the principle of parsimony—not overcomplicating or over-engineering a solution. It's absolutely critical here that we don't need to build a crazily large and complex codebase. We have a very specific goal, which should not require an excess amount of infrastructure and code at this stage, and only includes well-testable, understandable modules. It's important for you to understand that we want to focus on building and iterating quickly while documenting and communicating for review and planning extensively. As such, you must ensure that this project fits in precisely with the requirements, this transcript outlining my vision, and also the overall overarching goal and objective for my Research.
