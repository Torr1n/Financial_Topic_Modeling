# Raw Transcript: Cloud Migration for Financial Topic Modeling Pipeline

**Source:** Senior Researcher Vision Session
**Date:** November 2024
**Topic:** MVP to Cloud-Native Pipeline Migration

---

Okay, so here's some background. I am an undergraduate student in my final year, working as an undergraduate research assistant for a professor in the finance department, with collaborations with computer science faculty members in the field of natural language processing. My undergraduate degree is in finance, computer science, and statistics.

Essentially, what I'm building in this project is my capstone. This is academic work that I intend to publish and provide on GitHub for downstream use. I'm also working on it with a team, so code quality, clarity, thinking through and structuring our ideas, documenting our reasoning, key decisions, etc., and visualizing the architecture are all important ideas here.

Given that background, the main purpose of this entire repository is to take this MVP that I built with a partner over the summer—mainly on the side, not really with any intentions of doing anything except for presenting it as a part of a class—and now translate it into something that is production-ready, scalable, and most importantly, ready to be hosted on the cloud.

So, we want to essentially create an earnings call theme identification pipeline that does cloud-native batch processing, such that our financial sentiment analysis is actually scalable. While this pipeline and the local BertTopic MVP repository does include sentiment analysis and event study subfolders, all that we're going to be focusing on here is scaling up our theme identification pipeline to the cloud. We're not going to focus on any of the downstream sentiment analysis or event study at this point.

I also want to emphasize *very* early on here - the `Local_BERTopic_MVP` is **POORLY STRUCTURED AND DOCUMENTED CODE** from a time when we were iterating fast to build out a proof-of-concept of this system for a tight-deadlined course project last year. This was coded mostly with a noticeably older, inferior model of Claude. I expect better from you and that you use *only the ideas, specifications and data flow* from the first MVP should inform our solution here. You should still investigate this code to understand it, but I want you to be aware there are functions or logic paths that are untested and not used in the pipeline.

The README in this MVP essentially goes over the pipeline, but the main modules are data ingestion, firm-level topic modeling, and then cross-firm theme identification. Currently, we ran this MVP on a CSV of a single quarter of transcripts. You can see this CSV in the repository. And even with our single quarter of earnings calls across a thousand firms, we still had to rely on running this local BertTopic MVP across multiple team members' computers with multiple instances of the script running on different subsets of the firms in the CSV file. Really, that limitation is what led us to decide that in order to do the type of analysis that we aspire to in the future, where we can look at trends over time involving multiple quarters, the obvious solution is to migrate from this MVP to a production-ready, cloud-hosted solution that can scale to over 10,000 firms and multiple quarters.

The most obvious separation that I see and pattern that I think we should utilize is a map and then reduce step. In the sense that we have these firm-level topic models that are independent and write their results to intermediary files, which is how we actually did our analysis across multiple computers for our baseline single-quarter transcript CSV. And then from these temporary files, we then aggregate them in a reduce step where we run a secondary topic modeling step on the topics themselves.

As such, I'm going to get into my rough analysis of what types of cloud resources I would think would be best to use here. But I really want you to keep in mind that this is, in addition to being a production research task, also an educational exercise for myself in terms of building large-scale machine learning pipelines on the cloud. While I am a certified cloud practitioner and have some experience with the cloud, this is certainly my most complex project to date. And I want you to push my understanding, my design, and my understanding to come up with not just the best solution, but also the one that balances complexity and quality. I don't have months and months and months to build this system; I have just over a week of intensive work. And at the end of the day, I just need it to work. So, tradeoffs between really high complexity for small optimizations might be noted and left for the future.

That being said, based off this map and reduce step that I've seen, with the obvious final output being a database of a schema linking multiple tables of firm topics, cross-firm themes, and individual firm sentences for downstream querying and use in our theme sentiment analysis and event study. For the parallel firm-level processing, because this is an inherently batch process, I would use AWS Batch with spot instances because this is less of a latency concern and more of a cost concern. With these AWS Batch instances feeding their output into a temporary S3 bucket, which is then used in the cross-firm aggregation, which I believe the best choice to use is a SageMaker instance, which then produces a queryable hierarchical database linking themes to topics to source sentences and firms. In the sense that it can be used for downstream analysis. For this, I would obviously choose a DynamoDB, or perhaps not obviously, but it would be my suspicion.

So the full system design or architecture snapshot would be to use AWS Step Functions distributed map with a maximum number of jobs for orchestration. Then using AWS Batch array with large spot instances and a container that has dependencies on BertTopic, sentence-transformers, and PyTorch, trying to be as minimal as possible. There perhaps could be some other dependencies, but one thing to consider here is that right now we would just want to test it on our one CSV that we have here. But in the future, we would want to use the Words connector to query portions of the input firms. The processing step would be to load the transcript for this specific firm and earnings call. So each one of these containers would be processing a single firm, embedding them using an embedding model. I'm not sure what the best way to do that is, if we should have some shared embedding instance that gets called by each of our small containers or if it's done within the containers. And then we write the output topics to S3.

In the reduce phase, I think the process would be to load the topics from the intermediate storage, re-embed the topic names, do our BertTopic step to find cross-firm themes, and then write the results to DynamoDB with the hierarchy of themes, then firm topics, then firms. For this, I would want to use SageMaker with probably a larger, or surely a larger instance, that I would expect to take longer than the map phase.

There would be some future integrations that we would want to make in terms of having an LLM call for sentence filtering. This is in the original spec, along with another LLM call for stop-word generation. But in this sequential developmental process, our first focus is getting the cloud architecture built out and functional on the core task here, and then we can add the LLM API calls in afterwards, noting where we had to use them for sentence filtering, stop-word generation, topic naming, and theme naming, which are non-critical.

So what I first want you to do is I want you to work with me through the architecture, figuring out based off of my needs, what I've discussed here, as well as the local BertTopic MVP, what is feasible for the cloud architecture, focusing on minimizing cost and complexity while meeting our functional requirements. The ultimate goal is to be able to run our pipeline on the original CSV and measure the time it takes to process that single quarter of data as well as the cost.

Another consideration that we should also have in mind is to design everything modularly. As a part of this research, I intend to get more funding to have access to the Words tables again, rather than relying on local CSV files. So we should consider integrations and modularity at the data ingestion layer. And then also in terms of our topic modeling unit, in the sense of both our theme identification and our firm-level topic modeling, these should use some shared topic model that is a separate module because the whole point of this research is to also compare different topic modeling methods on these quarterly data sets. So using neural topic modeling methods, LDA, and others, along with our BertTopic topic model that we have in our MVP, and we should design our code with this in mind to make that future process as seamless as possible. It's important that we figure out what exact containers we will need, such that we can test locally and iterate in some way that provides validation and feedback before we get to the step of running the whole pipeline. When we get to the point that we run the full pipeline, we should have no doubt in mind about its ability to achieve our goal through detailed test-driven development and local testing when available.

So what I want to focus on is a specification for these two core containers or EC2 instances that we will be creating for the map phase and for the reduce phase, with connections for data ingestion, topic modeling in the map phase, and S3 intermediate file reading, and cross-firm topic modeling in the, and DynamoDB writing in the second, reduce instance. I want to use an infrastructure as a code tool for this, Terraform, such that we can have everything regarding this project available in one repository and auditable by other team members or faculty.

It's also really important that we create something that is well-structured, understandable, and well-tested, following test-driven development principles. We want to ask the "why," determine what needs to be implemented, and determine the public signatures that need to be implemented—what these interfaces will take in and what we expect for them to receive. We will write tests for that behavior, following the test-driven development procedure, and then implement the necessary public methods and helper functions to achieve the functionality that we expect. We will iterate on bugs and also provide timely reviews where you present your work to me, I validate the functionality, and we continue. This also allows me to provide your work to OpenAI Codex, which is another coding agent whose job is to have a neutral, impartial perspective on your work and analyze your plans and review your work based on the overall project goals and the specific package we're working with here.

The final deliverable should be the code and containers for these map and reduce steps, as well as the full Terraform for creating the cloud infrastructure we need, with it running and populating our DynamoDB with the themes, topics, and firms from our baseline transcript here. We want to build in small, granular tasks and define an overarching specification or manifesto for this project before we get into any work. So it's important that we do our due diligence to be as thorough as possible in the planning, architecting, and design phase, going back and forth, ensuring my learning and understanding before we get into the actual code and testing, following test-driven development.

So, those are the only goals here. It is not necessary to overcomplicate, but it is necessary to build clean, well-written, understandable, modular code designed to be looked at by other members of this development team and potentially built upon. Think about if another claude instance was using this as a basis for a downstream refactor or new feature. What about when I publish this? Or have my faculty supervisory committee review my work? So, let's make sure we do good work here that is transparent, understandable, observable, and something that we can be proud of.

The best engineers I've worked with write code my mom could read. They choose boring technology, they over-document the "why," and under-engineer the "how." Complexity is not a flex; it becomes a liability. Real seniority is making hard problems look simple, not making simple problems look hard. And we want to embody that within our design here. We are building using validation-as-we-go with test-driven development where possible to anticipate and address issues as we go, working with Codex, and we don't want to run into an excess of errors as we're building out this cloud integration and extension of our first MVP, which was minimally viably working

Finally, once you've gone through this process of viewing and understanding the reports of any sub-agents and going and verifying their findings yourself, think deeply to synthesize everything, not only from their reports but going over again everything in this task package to come up with a comprehensive plan that also follows the principle of parsimony—not overcomplicating or over-engineering a solution. It's absolutely critical here that we don't need to build a crazily large and complex codebase. We have a very specific goal, which should not require an excess amount of infrastructure and code at this stage, and only includes well-testable, understandable modules. It's important for you to understand that we want to focus on building and iterating quickly while documenting and communicating for review and planning extensively. As such, you must ensure that this project fits in precisely with the requirements, this transcript outlining my vision, and also the overall overarching goal and objective for my Research.
